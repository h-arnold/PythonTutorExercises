#!/usr/bin/env python3
"""Scaffold a new exercise (notebook + tests).

Usage:
  python scripts/new_exercise.py ex001 "Variables and Types" --slug variables_and_types
    python scripts/new_exercise.py ex010 "Week 1" --slug week1 --parts 3

This creates:
  exercises/ex001_variables_and_types/README.md
  notebooks/ex001_variables_and_types.ipynb
    tests/test_ex001_variables_and_types.py
"""

from __future__ import annotations

import argparse
import datetime as _dt
import json
import re
import uuid
from pathlib import Path
from typing import Any

ROOT = Path(__file__).resolve().parents[1]


def _slugify(text: str) -> str:
    text = text.strip().lower()
    text = re.sub(r"[^a-z0-9]+", "_", text)
    text = re.sub(r"_+", "_", text).strip("_")
    if not text:
        raise SystemExit("Slug is empty; provide --slug or a better title.")
    return text


def _make_notebook_with_parts(
    title: str, *, parts: int, exercise_type: str | None = None
) -> dict[str, Any]:
    if parts < 1:
        raise ValueError("parts must be >= 1")

    def _meta(language: str, *, tags: list[str] | None = None) -> dict[str, Any]:
        meta: dict[str, object] = {"id": uuid.uuid4().hex[:8], "language": language}
        if tags:
            meta["tags"] = tags
        return meta

    cells: list[dict[str, Any]] = [
        {
            "cell_type": "markdown",
            "metadata": _meta("markdown"),
            "source": [
                f"# {title}\n",
                "\n",
                "## Goal\n",
                "Complete each exercise cell, then run the tests.\n",
                "\n",
                "## How to work\n",
                "- Write your solution(s) in the exercise cell(s)\n",
                "- Run `pytest -q`\n",
            ],
        }
    ]

    # For debug exercises we scaffold a short expected-output cell, the buggy
    # tagged student cell, and an explanation cell for each part. For other
    # exercise types we keep the existing simple layout.
    if exercise_type == "debug":
        for i in range(1, parts + 1):
            ex_tag = f"exercise{i}"
            expl_tag = f"explanation{i}"

            # Expected behaviour / expected output cell
            cells.append(
                {
                    "cell_type": "markdown",
                    "metadata": _meta("markdown"),
                    "source": [
                        f"# Exercise {i} — Expected behaviour\n",
                        "Describe what the corrected program should output.\n",
                        "### Expected output\n",
                        "```\n",
                        "(put example output here)\n",
                        "```\n",
                    ],
                }
            )

            # Buggy implementation (tagged for students to edit)
            cells.append(
                {
                    "cell_type": "code",
                    "metadata": _meta("python", tags=[ex_tag]),
                    "execution_count": None,
                    "outputs": [],
                    "source": [
                        "# BUGGY IMPLEMENTATION (students edit this tagged cell)\n",
                        "def solve() -> object:\n",
                        '    """Return the correct result for this exercise."""\n',
                        "    return 'TODO'\n",
                    ],
                }
            )

            # What actually happened — explanation cell (tagged)
            cells.append(
                {
                    "cell_type": "markdown",
                    "metadata": _meta("markdown", tags=[expl_tag]),
                    "source": [
                        "### What actually happened\n",
                        "Describe briefly what happened when you ran the code (include any error messages or incorrect output).\n",
                    ],
                }
            )

    else:
        # Non-debug behaviour: keep existing single/multi-part layout
        if parts == 1:
            tag = "exercise1"
            cells.append(
                {
                    "cell_type": "code",
                    "metadata": _meta("python", tags=[tag]),
                    "execution_count": None,
                    "outputs": [],
                    "source": [
                        "# Exercise 1\n",
                        "# The tests will execute the code in this cell.\n",
                        "\n",
                        "def solve() -> object:\n",
                        '    """Return the correct result for the exercise."""\n',
                        "    return 'TODO'\n",
                    ],
                }
            )
        else:
            for i in range(1, parts + 1):
                tag = f"exercise{i}"
                cells.append(
                    {
                        "cell_type": "markdown",
                        "metadata": _meta("markdown"),
                        "source": [
                            f"## Exercise {i}\n",
                            "(Write the prompt here.)\n",
                        ],
                    }
                )
                cells.append(
                    {
                        "cell_type": "code",
                        "metadata": _meta("python", tags=[tag]),
                        "execution_count": None,
                        "outputs": [],
                        "source": [
                            f"# Exercise {i}\n",
                            "# The tests will execute the code in this cell.\n",
                            "def solve() -> object:\n",
                            '    """Return the correct result for this exercise."""\n',
                            "    return 'TODO'\n",
                        ],
                    }
                )

    cells.append(
        {
            "cell_type": "code",
            "metadata": _meta("python"),
            "execution_count": None,
            "outputs": [],
            "source": [
                "# Optional self-check (not graded)\n",
                "# You can run small experiments here.\n",
            ],
        }
    )

    return {
        "cells": cells,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3",
            },
            "language_info": {"name": "python", "version": "3"},
        },
        "nbformat": 4,
        "nbformat_minor": 5,
    }


def _validate_and_parse_args() -> argparse.Namespace:
    """Parse and validate command-line arguments."""
    parser = argparse.ArgumentParser(description="Create a new exercise skeleton")
    parser.add_argument("id", help='Exercise id like "ex001"')
    parser.add_argument("title", help="Human title for the exercise")
    parser.add_argument(
        "--slug",
        help="Optional slug (snake_case). Defaults to slugified title.",
        default=None,
    )
    parser.add_argument(
        "--parts",
        type=int,
        default=1,
        help="How many graded exercise cells to scaffold in the notebook (default: 1).",
    )
    parser.add_argument(
        "--type",
        choices=["debug", "modify", "make"],
        default=None,
        help="Optional exercise type; when set to 'debug' the scaffold includes expected-output and explanation cells.",
    )
    args = parser.parse_args()

    if args.parts < 1:
        raise SystemExit("--parts must be >= 1")
    if args.parts > 20:
        raise SystemExit("--parts is capped at 20 to keep notebooks manageable")

    ex_id = args.id.strip().lower()
    if not re.fullmatch(r"ex\d{3}", ex_id):
        raise SystemExit('Exercise id must look like "ex001".')

    slug = args.slug.strip().lower() if args.slug else _slugify(args.title)
    if not re.fullmatch(r"[a-z0-9]+(?:_[a-z0-9]+)*", slug):
        raise SystemExit("Slug must be snake_case containing only a-z, 0-9, and underscores.")

    return args


def _check_exercise_not_exists(exercise_key: str) -> None:
    """Raise SystemExit if exercise already exists."""
    ex_dir = ROOT / "exercises" / exercise_key
    nb_path = ROOT / "notebooks" / f"{exercise_key}.ipynb"
    nb_solution_path = ROOT / "notebooks" / "solutions" / f"{exercise_key}.ipynb"
    test_path = ROOT / "tests" / f"test_{exercise_key}.py"

    if ex_dir.exists() or nb_path.exists() or nb_solution_path.exists() or test_path.exists():
        raise SystemExit(f"Exercise already exists: {exercise_key}")


def main() -> int:
    args = _validate_and_parse_args()

    slug = args.slug.strip().lower() if args.slug else _slugify(args.title)
    exercise_key = f"{args.id.strip().lower()}_{slug}"

    _check_exercise_not_exists(exercise_key)

    ex_dir = ROOT / "exercises" / exercise_key
    nb_path = ROOT / "notebooks" / f"{exercise_key}.ipynb"
    nb_solution_path = ROOT / "notebooks" / "solutions" / f"{exercise_key}.ipynb"
    test_path = ROOT / "tests" / f"test_{exercise_key}.py"

    ex_dir.mkdir(parents=True)
    (ex_dir / "__init__.py").write_text("\n", encoding="utf-8")

    today = _dt.date.today().isoformat()

    (ex_dir / "README.md").write_text(
        "\n".join(
            [
                f"# {args.title}",
                "",
                "## Student prompt",
                "- Open the matching notebook in `notebooks/`.",
                "- Write your solution in the notebook cell tagged `exercise1` (or `exercise2`, …).",
                "- Run `pytest -q` until all tests pass.",
                "",
                "## Teacher notes",
                f"- Created: {today}",
                "- Target concepts: (fill in)",
            ]
        )
        + "\n",
        encoding="utf-8",
    )

    test_lines: list[str] = [
        "from __future__ import annotations",
        "",
        "import pytest",
        "",
        "from tests.notebook_grader import exec_tagged_code",
        "",
        "",
        "def _run(tag: str):",
        f"    ns = exec_tagged_code('notebooks/{exercise_key}.ipynb', tag=tag)",
        "    assert 'solve' in ns, 'Student cell must define solve()'",
        "    result = ns['solve']()",
        "    # Placeholder guard: student must change the scaffold",
        "    assert result != 'TODO'",
        "    return result",
        "",
        "",
    ]

    if args.parts == 1:
        test_lines += [
            "def test_student_cell_runs() -> None:",
            "    _run('exercise1')",
            "",
        ]
    else:
        tags = ", ".join([f"'exercise{i}'" for i in range(1, args.parts + 1)])
        test_lines += [
            f"@pytest.mark.parametrize('tag', [{tags}])",
            "def test_exercise_cells_run(tag: str) -> None:",
            "    _run(tag)",
            "",
        ]

    # If this is a debug exercise, add tests that assert students filled the
    # `explanationN` markdown cells with meaningful content (>10 characters).
    if args.type == "debug":
        test_lines += [
            "# Explanation cell checks for debug exercises",
            "import json",
            "",
            "def _get_explanation(notebook_path: str, tag: str = 'explanation1') -> str:",
            "    nb = json.load(open(notebook_path, 'r', encoding='utf-8'))",
            "    for cell in nb.get('cells', []):",
            "        tags = cell.get('metadata', {}).get('tags', [])",
            "        if tag in tags:",
            "            return ''.join(cell.get('source', []))",
            "    raise AssertionError(f'No explanation cell with tag {tag}')",
            "",
        ]
        if args.parts == 1:
            test_lines += [
                "def test_explanation_has_content() -> None:",
                f"    explanation = _get_explanation('notebooks/{exercise_key}.ipynb', tag='explanation1')",
                "    assert len(explanation.strip()) > 10, 'Explanation must be more than 10 characters'",
                "",
            ]
        else:
            test_lines += [
                "import pytest",
                f"TAGS = [f'explanation{{i}}' for i in range(1, {args.parts} + 1)]",
                "@pytest.mark.parametrize('tag', TAGS)",
                "def test_explanations_have_content(tag: str) -> None:",
                f"    explanation = _get_explanation('notebooks/{exercise_key}.ipynb', tag=tag)",
                "    assert len(explanation.strip()) > 10, 'Explanation must be more than 10 characters'",
                "",
            ]

    test_path.write_text("\n".join(test_lines), encoding="utf-8")

    # Build notebook with the optional exercise type (e.g., debug)
    notebook = _make_notebook_with_parts(args.title, parts=args.parts, exercise_type=args.type)
    nb_path.write_text(json.dumps(notebook, indent=2), encoding="utf-8")

    nb_solution_path.parent.mkdir(parents=True, exist_ok=True)
    nb_solution_path.write_text(json.dumps(notebook, indent=2), encoding="utf-8")

    # If this is a debug exercise, update README to mention explanation tags
    if args.type == "debug":
        readme_lines = (ex_dir / "README.md").read_text(encoding="utf-8").splitlines()
        # Add short instruction about the explanation cell
        readme_lines.insert(
            7,
            "- After running your corrected solution, describe what happened in the cell tagged `explanation1` (or `explanationN`).",
        )
        (ex_dir / "README.md").write_text("\n".join(readme_lines) + "\n", encoding="utf-8")

    print(f"Created exercise: {exercise_key}")
    print(f"- {ex_dir.relative_to(ROOT)}")
    print(f"- {nb_path.relative_to(ROOT)}")
    print(f"- {nb_solution_path.relative_to(ROOT)}")
    print(f"- {test_path.relative_to(ROOT)}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
